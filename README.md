# Reinforcement Learning : √âtude Comparative PPO vs SAC (Hopper-v5)

Ce projet, r√©alis√© dans le cadre du module **IMDS5A** √† **Polytech Clermont-Ferrand**, explore l‚Äôapplication de deux algorithmes majeurs d‚Äôapprentissage par renforcement profond ‚Äî **Proximal Policy Optimization (PPO)** et **Soft Actor-Critic (SAC)** ‚Äî pour le contr√¥le continu d‚Äôun robot monopode instable dans l‚Äôenvironnement **Hopper-v5 (MuJoCo)**.

---

## üéØ Probl√©matique

**Comment apprendre √† un robot physiquement instable √† sauter et courir de mani√®re autonome sans tomber ?**

Ce projet compare :
- une approche **prudente et stable** (*PPO*),
- √† une m√©thode **rapide et fortement exploratrice** (*SAC*),

afin d‚Äôidentifier la strat√©gie la plus performante dans un environnement dynamique et chaotique.

---

## üõ†Ô∏è Algorithmes Impl√©ment√©s

### 1. Proximal Policy Optimization (PPO)
- **Type** : On-policy  
- **Architecture** : Actor-Critic d√©coupl√©s (MLP 2√ó64 neurones)  
- **M√©canisme cl√©** : Clipping des mises √† jour pour garantir une optimisation stable  


---

### 2. Soft Actor-Critic (SAC)
- **Type** : Off-policy  
- **Architecture** : R√©seaux profonds (MLP 2√ó256 neurones) avec *Twin Critics*  
- **M√©canisme cl√©** : Maximisation de l‚Äôentropie pour encourager l‚Äôexploration  
- **Optimisation** : R√©utilisation efficace des donn√©es via un *Replay Buffer*

---

## üíª Installation et Utilisation

### Pr√©requis
- Python 3.8+
- PyTorch (support **CUDA recommand√©**)
- Gymnasium `[mujoco]`
- MuJoCo

### Installation
```bash
pip install gymnasium[mujoco] torch numpy matplotlib
import gymnasium as gym

# Charger l'environnement
env = gym.make("Hopper-v5")

# Configurer les hyperparam√®tres
# Batch size : 64 (PPO), 256 (SAC)
# Lancer l'entra√Ænement via le jupyter notebook 
```

## üë• Auteurs

- **Ayman ZEJLI**
- **Lo√Øc MAGNAN**

**Encadrant** : *Julien Hautot*  
**Institution** : Polytech Clermont-Ferrand ‚Äî IMDS5A
